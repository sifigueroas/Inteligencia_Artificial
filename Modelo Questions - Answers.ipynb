{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6ff614",
   "metadata": {},
   "source": [
    "# Fine-tune Spanish RoBERTa (BSC) for QA on SQAC / by mrm8488"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df446a03",
   "metadata": {},
   "source": [
    "Based on Sylvain Gugger Colab\n",
    "\n",
    "imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce7a0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9fd11551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b305c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bf2ecb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f779e03",
   "metadata": {},
   "source": [
    "Si est√° abriendo este cuaderno localmente, aseg√∫rese de que su entorno tenga una instalaci√≥n de la √∫ltima versi√≥n de esas bibliotecas.\n",
    "\n",
    "Para poder compartir su modelo con la comunidad y generar resultados como el que se muestra en la imagen a continuaci√≥n a trav√©s de la API de inferencia, hay algunos pasos m√°s a seguir.\n",
    "\n",
    "Primero debe almacenar su token de autenticaci√≥n del sitio web Hugging Face (reg√≠strese aqu√≠ si a√∫n no lo ha hecho). Luego ejecute la siguiente celda e ingrese su nombre de usuario y contrase√±a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3a04d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "437b97e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb761eed9cac469d9d8cdd06475631b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec159cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c730a27",
   "metadata": {},
   "source": [
    "Necesitamos instalar Git-LFS. Descomenta y ejecuta la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8990d1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"apt\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "! apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd884a",
   "metadata": {},
   "source": [
    "Aseg√∫rese de que su versi√≥n de Transformers sea al menos 4.11.0 ya que la funcionalidad se introdujo en esa versi√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7da1c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab19588",
   "metadata": {},
   "source": [
    "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c16936",
   "metadata": {},
   "source": [
    "# Fine-tuning de un modelo para la tarea de QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2cf8b",
   "metadata": {},
   "source": [
    "En este cuaderno, veremos c√≥mo hacer \"fine-tuning\" a uno de los modelos de ü§ó Transformers para la tarea de respuesta a una pregunta (QA), que es la tarea de extraer la respuesta a una pregunta de un contexto dado. Veremos c√≥mo cargar f√°cilmente un conjunto de datos para este tipo de tareas y usar la API Trainer para ajustar un modelo en √©l."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299f544",
   "metadata": {},
   "source": [
    "** Nota: ** Este cuaderno afina los modelos que responden preguntas tomando una subcadena de un contexto, no generando texto nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dee38e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_v2 = False\n",
    "model_checkpoint = \"BSC-TeMU/roberta-base-bne\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920da564",
   "metadata": {},
   "source": [
    "### Cargando el dataset\n",
    "Usaremos la biblioteca ü§ó Datasets para descargar los datos y obtener la m√©trica que necesitamos usar para la evaluaci√≥n (para comparar nuestro modelo con el benchmark). Esto se puede hacer f√°cilmente con las funciones load_dataset yload_metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e2c85461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d71595",
   "metadata": {},
   "source": [
    "Para nuestro ejemplo usaremos el Dataset SQAC. El cuaderno debe funcionar con cualquier conjunto de datos de respuesta a preguntas proporcionado por la biblioteca ü§ó Datasets. Si est√° utilizando su propio conjunto de datos definido a partir de un archivo JSON o csv (consulte la documentaci√≥n de Datasets[texto del enlace](https://) sobre c√≥mo cargarlos ), es posible que necesite algunos ajustes en los nombres de las columnas utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "18e0979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sqac (C:/Users/sifigueroa/.cache/huggingface/datasets/BSC-TeMU___sqac/SQAC/0.0.0/039a4db4240c2eeaeb62497c826a49721ea57dffae7eeff93a8179fcdc5fd9fe)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf918d1349344709aa17a547af8c5bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"BSC-TeMU/SQAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9794f7",
   "metadata": {},
   "source": [
    "The datasets object itself is DatasetDict, which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d93478d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 15036\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1864\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1910\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c5ea1afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6cf3dcd6-b5a3-4516-8f9e-c5c1c6b66628',\n",
       " 'title': 'Historia de Jap√≥n',\n",
       " 'context': 'La historia de Jap√≥n (Êó•Êú¨„ÅÆÊ≠¥Âè≤ o Êó•Êú¨Âè≤, Nihon no rekishi / Nihonshi?) es la sucesi√≥n de hechos acontecidos dentro del archipi√©lago japon√©s. Algunos de estos hechos aparecen aislados e influenciados por la naturaleza geogr√°fica de Jap√≥n como naci√≥n insular, en tanto que otra serie de hechos, obedece a influencias for√°neas como en el caso del Imperio chino, el cual defini√≥ su idioma, su escritura y, tambi√©n, su cultura pol√≠tica. Asimismo, otra de las influencias for√°neas fue la de origen occidental, lo que convirti√≥ al pa√≠s en una naci√≥n industrial, ejerciendo con ello una esfera de influencia y una expansi√≥n territorial sobre el √°rea del Pac√≠fico. No obstante, dicho expansionismo se detuvo tras la Segunda Guerra Mundial y el pa√≠s se posicion√≥ en un esquema de naci√≥n industrial con v√≠nculos a su tradici√≥n cultural.',\n",
       " 'question': '¬øQu√© influencia convirti√≥ Jap√≥n en una naci√≥n industrial?',\n",
       " 'answers': {'text': ['la de origen occidental'], 'answer_start': [473]}}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee8284",
   "metadata": {},
   "source": [
    "Podemos ver que las respuestas est√°n indicadas por su posici√≥n inicial en el texto (aqu√≠ en el car√°cter 473) y su texto completo, que es una subcadena del contexto como mencionamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d089d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"No puedes seleccionar m√°s elementos que los que contiene el dataset\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6e9a89f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b292f4c-822f-4868-8f9d-50c883946c73</td>\n",
       "      <td>CESS-CAST-P_122_19990701_rec.txt</td>\n",
       "      <td>El montaje de Joglars sobre el pintor se estrenar√° el 6 de septiembre en Figueres. El √∫ltimo delirio de Dal√≠ ocurri√≥ justo antes de morir. En un segundo, toda la vida del artista pas√≥ ante sus ojos como en una pel√≠cula. As√≠ es como la compa√±√≠a teatral Joglars recrea la agon√≠a de Dal√≠, tema central de Dal√≠, el pr√≥ximo espect√°culo del grupo que dirige Albert Boadella. El director teatral clausur√≥ ayer el taller El actor como creador en la Universidad Internacional Men√©ndez Pelayo de Santander con una tertulia sobre el montaje de la obra. Para el dramaturgo, tras la imagen fr√≠vola de Dal√≠, que el mismo artista contribuy√≥ a fomentar, se encuentra \"no s√≥lo un pintor excepcional, sino tambi√©n un escritor remarcable y un aut√©ntico provocador\". Dal√≠, afirm√≥ Boadella, fue \"un ni√±o que nunca lleg√≥ a hacerse adulto\", lo que explica, a su juicio, su \"egocentrismo y su sexualidad\". La obra se estrenar√° el 6 de septiembre, en Figueres, la ciudad preferida del genio.</td>\n",
       "      <td>¬øQu√© representa la obra?</td>\n",
       "      <td>{'text': ['la agon√≠a de Dal√≠'], 'answer_start': [267]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cbae1ee-987e-412b-ab21-ae7623026b56</td>\n",
       "      <td>Chacas</td>\n",
       "      <td>Medios de comunicaci√≥n \\nEl servicio de telefon√≠a m√≥vil tiene presencia significativa. Son tres las operadoras m√≥viles que tienen cobertura en la ciudad, siendo Bitel y Claro las que cuentan con la red 4G,. As√≠ mismo, el pueblo cuenta con el tendido de fibra √≥ptica perteneciente a la red dorsal que se instal√≥ durante el gobierno de Ollanta Humala. Sin embargo a√∫n no est√° operativa, se estima que para 2025 Chacas se integrar√° finalmente a la red dorsal de fibra √≥ptica. Por otro lado, se pueden sintonizar seis estaciones de radio, y tres canales de televisi√≥n anal√≥gica y televisi√≥n por sat√©lite. El distrito cuenta con una radio municipal, que ofrece informaci√≥n y noticias relevantes sobre la provincia.</td>\n",
       "      <td>¬øQu√© compa√±√≠as m√≥viles dan cobertura 4G a Chacas?</td>\n",
       "      <td>{'text': ['Bitel y Claro'], 'answer_start': [160]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>968d27c8-8a4e-4906-988b-de43ab166b81</td>\n",
       "      <td>Ir√°n sigue reprimiendo las protestas y eleva la tensi√≥n con el Reino Unido</td>\n",
       "      <td>24 de junio de 2009 24 de junio de 2009Teher√°n, Ir√°n  ‚Äî El gobierno iran√≠ ha impedido a entre doscientos y un millar de manifestantes, convocados por el ex-primer ministro Mir Husein Musavi contra la reelecci√≥n del actual presidente Mahmud Ahmadineyad, que pudieran manifestarse frente a la sede del Parlamento en Teher√°n. Cientos de polic√≠as apoyados por milicianos basiy√≠s, armados √©stos de porras y barras de hierro, han dispersado a la multitud con gases lacrim√≥genos, seg√∫n testimonios de testigos. Respecto a estas protestas, el L√≠der Supremo iran√≠, Al√≠ Jamenei, ha indicado que \"Insisto y volver√© a insistir en aplicar la ley en este asunto. Ni el gobierno ni la naci√≥n se rendir√°n a la presi√≥n a ning√∫n precio\", cerrando de esta manera la posibilidad de cualquier entendimiento con los reformistas. √âstos siguen presionando al r√©gimen iran√≠. Uno de los m√°s representativos reformistas, el cl√©rigo Husein Al√≠ Montazer√≠, en arresto domiciliario desde hace diez a√±os en la ciudad santa chi√≠ de Qom, ha convocado tres d√≠as de luto por las v√≠ctimas de la represi√≥n gubernamental y ha indicado que \"deso√≠r las demandas del pueblo est√° prohibido por la religi√≥n\". El candidato en las pasadas elecciones del 12 de junio,  Mehdi Karrub√≠, ha rechazado los resultados electorales y ha manifestado que el gobierno resultante de los mismos ser√° ileg√≠timo y ha pedido la anulaci√≥n de los mismos. Por su parte, la esposa de Musavi,  Zahra Rahnavard, ha pedido al gobierno la liberaci√≥n de los pol√≠ticos y personas detenidas durante las manifestaciones indicando que no se puede tratar a sus seguidores como \"si la ley marcial se hubiese impuesto en las calles\". Entretanto, el Consejo de Guardianes, ha denegado la posibilidad de la anulaci√≥n de las votaciones, y aunque ha reconocido que en algunas circunscripciones han votado m√°s personas que las registradas para hacerlo, ha comentado que no hay graves irregularidades, por lo que Ahmadineyad podr√≠a ser jurado en su cargo a mediados de agosto. Por otro lado, el gobierno de Ir√°n ha incrementado el nivel de las acusaciones a gobiernos extranjeros, fundamentalmente los del Reino Unido y Estados Unidos, de fomentar un complot con el fin de derrocar el r√©gimen iran√≠. En este contexto, el ministro iran√≠ de Asuntos Exteriores, Manoucher Mottaki, ha confirmado la expulsi√≥n de dos diplom√°ticos brit√°nicos y ha insinuado que Ir√°n va a replantearse el nivel de sus relaciones con el Reino Unido. En respuesta a esta decisi√≥n iran√≠, el gobierno brit√°nico de Gordon Brown ha ordenado asimismo la expulsi√≥n de su territorio de dos diplom√°ticos iran√≠es. El portavoz del gobierno brit√°nico se ha defendido de las acusaciones iran√≠es y ha comentado que la intenci√≥n de Ir√°n de pretender extender a otros un conflicto interno es \"rechazable y sin fundamento\". Asimismo, el ministro iran√≠ de Inteligencia, Gholam Husein Mohseni Ejei, ha anunciado que varios ciudadanos brit√°nicos han sido detenidos bajo la acusaci√≥n de fomentar disturbios en la capital.</td>\n",
       "      <td>¬øPor qu√© motivo ha organizado Musavi  estas protestas?</td>\n",
       "      <td>{'text': ['la reelecci√≥n del actual presidente Mahmud Ahmadineyad'], 'answer_start': [197]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61913f0b-f561-4669-858a-9d3c4b131992</td>\n",
       "      <td>Ignacio de Antioqu√≠a</td>\n",
       "      <td>Ignacio de Antioqu√≠a (en griego: ·º∏Œ≥ŒΩŒ¨œÑŒπŒøœÇ ·ºàŒΩœÑŒπŒøœáŒµŒØŒ±œÇ) (Siria, Imperio romano, 35 - Roma, entre 108 y 110)  es uno de los padres de la Iglesia y, m√°s concretamente, uno de los padres apost√≥licos por su cercan√≠a cronol√≥gica con el tiempo de los ap√≥stoles.‚Äã Fue el primero en llamar Cat√≥lica a la Iglesia.‚Äã Es autor de siete cartas que redact√≥ en el transcurso de unas pocas semanas, mientras era conducido desde Siria a Roma para ser ejecutado o, como √©l mismo escribi√≥:</td>\n",
       "      <td>¬øCu√°l fue el papel de Ignacio de Antioqu√≠a en los inicios del catolicismo?</td>\n",
       "      <td>{'text': ['es uno de los padres de la Iglesia y, m√°s concretamente, uno de los padres apost√≥licos por su cercan√≠a cronol√≥gica con el tiempo de los ap√≥stoles'], 'answer_start': [107]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bd29bc9b-8c3e-4c48-abe1-45ccee0e7263</td>\n",
       "      <td>AA_4723_20001006_&amp;_A_14423_20000317_&amp;_P_106_20000201_&amp;_P_111_20010102_&amp;_</td>\n",
       "      <td>El corredor italiano Fabrizio Guidi (Francaise de Jeux) se ha impuesto al esprint en la cuarta, y √∫ltima, etapa del Giro de la Provincia de Lucca (Toscana), que se ha disputado entre las localidades de Marina di Carrara y Lucca, sobre 152 kil√≥metros. El tambi√©n italiano Alessandro Petacchi (Fassa Bortolo), que entr√≥ en la segunda posici√≥n en la etapa, se ha alzado con el triunfo final en la prueba, mientras que el espa√±ol Oscar Freire, vigente campe√≥n del mundo y que inici√≥ la jornada en la segunda plaza de la general, abandon√≥ a poco de iniciada la etapa. Una sinuosa etapa, con seis vueltas a un duro circuito, en el que la gran dificultad representaba la subida a San Martino in Vignola en Vignola (4,1 kil√≥metros al 4 por ciento desnivel), que result√≥ muy movida, con continuos intentos de escapada y donde se dej√≥ ver en varias ocasiones el espa√±ol Manuel Beltr√°n, del Mapei. Al final, en el √∫ltimo paso por el San Martino in Vignola in Vignola, a unos catorce kil√≥metros de la meta, se defini√≥ ya definitivamente el grupo que se iba a disputar el triunfo en el esprint final. Un desenlace final en el que Fabrizio Guidi fue el m√°s r√°pido, logrando su cuarta victoria de temporada, y precediendo a un Alessandro Petacchi que entr√≥ en la segunda posici√≥n con la tranquilidad de saber que el triunfo en la general ya nadie se lo pod√≠a quitar tras la retirada de Freire, al que tan s√≥lo sacaba tres segundos.</td>\n",
       "      <td>¬øQu√© le ocurri√≥ a Oscar Freire?</td>\n",
       "      <td>{'text': ['abandon√≥ a poco de iniciada la etapa'], 'answer_start': [525]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f67411c8-8b96-431c-8e64-c8a9133381eb</td>\n",
       "      <td>Castor</td>\n",
       "      <td>Utilidad comercial \\nLas pieles de castor eran intercambiadas en trueques por los nativos americanos en el siglo XVII para conseguir bienes europeos. Despu√©s eran enviadas a Gran Breta√±a y Francia, donde eran convertidas en prendas. La extensa cacer√≠a y captura de castores puso en peligro su supervivencia. No obstante, lleg√≥ un momento en el que el comercio de pieles decay√≥ debido a su demanda decreciente en Europa y a la utilizaci√≥n de los terrenos de caza para apoyar al sector agr√≠cola en auge. Posteriormente se dar√≠a un peque√±o resurgimiento en la cacer√≠a de castores en algunas √°reas donde hab√≠a sobrepoblaci√≥n de estos animales; aunque por lo general la captura solo se realiza cuando la piel es valiosa, normalmente el resto del animal tambi√©n se utiliza como alimento para otros animales. La √∫nica piel en Am√©rica del Norte que superaba a la del castor en valor comercial era la del zorro rojo, la cual se dec√≠a que era cuarenta veces m√°s valiosa.</td>\n",
       "      <td>¬øCu√°l es el uso que se le da a los castores despellejados?</td>\n",
       "      <td>{'text': ['alimento para otros animales'], 'answer_start': [771]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>e36e6095-1d67-4866-b008-9872af1e1d36</td>\n",
       "      <td>CESS-CAST-P_136_20010501_rec.txt</td>\n",
       "      <td>El infierno del Atl√©tico en Segunda Divisi√≥n es especialmente duro para Kiko, precisamente el jugador que prest√≥ su imagen a la campa√±a publicitaria con la que el club madrile√±o busc√≥ el respaldo de su afici√≥n. El m√°s carism√°tico jugador rojiblanco hasta hace unos meses fue objeto el s√°bado por la noche de un intento de agresi√≥n a la salida del estadio, despu√©s de la humillante derrota ante el Murcia. Un grupo de los m√°s radicales miembros del Frente Atl√©tico le acus√≥ de ser el principal responsable del descenso y le reproch√≥ con suma dureza no colaborar econ√≥micamente con la pe√±a para sufragar sus desplazamientos. AMENAZAS E INSULTOS. El delantero jerezano est√° en el punto de mira de los ultras m√°s exaltados por no haber puesto las 150.000 pesetas que exigieron a cada futbolista para financiar el coste de un vuelo charter a Tenerife el pasado 8 de abril. En presencia de su padre, su mujer y su hija de nueve meses, que le esperaban dentro de un coche en la puerta cero del estadio, Kiko fue amenazado e insultado.</td>\n",
       "      <td>¬øCu√°ndo intentaron agredir a Kiko?</td>\n",
       "      <td>{'text': ['a la salida del estadio'], 'answer_start': [331]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>82b8a699-18e9-41b0-af93-bd2e6cdeb3a4</td>\n",
       "      <td>Dos pedidos de impedimento contra Lula llegan a la C√°mara de Diputados en Brasil</td>\n",
       "      <td>5 de agosto de 2005 Seg√∫n la Agencia C√°mara, dos pedidos de impedimento (impeachment) contra el Presidente de la Rep√∫blica Luiz In√°cio Lula da Silva llegaron a la C√°mara de Diputados de Brasil, este mi√©rcoles (3) en la noche. Los pedidos presentados fueron suscritos por dos abogados: Gildson Gomes de los Santos, de Ribera del Palomar, de Bah√≠a, y de Aylton Ferraz Monjas, de Guaruj√°, de S√£o Paulo. Seg√∫n la legislaci√≥n brasile√±a, el Presidente de la C√°mara de Diputados, Severino Cavalcanti, debe analizar los pedidos de impeachment y puede aceptarlos o rechazarlos, si considera que no son aplicables. Cavalcanti dijo: \"La asesor√≠a est√° estudiando y creo que en pocos d√≠as tendr√© una posici√≥n firmada sobre el asunto. Pero no har√© nada precipitado\". Por la Constituci√≥n Brasile√±a (art√≠culo 51) la C√°mara de los Diputados puede autorizar, con el apoyo de las dos terceras partes de sus integrantes, la instauraci√≥n de proceso de impedimento contra el Presidente de la Rep√∫blica, el Vicepresidente y los Ministros de Estado. Una vez autorizado, el proceso de impeachment y juicio del Presidente es llevado por el Senado Federal. El Senado, con el apoyo de dos tercios de los senadores y bajo el comando del Presidente del Supremo Tribunal Federal, puede condenar al Presidente con la p√©rdida del cargo e inhabilitaci√≥n por ocho a√±os para el ejercicio de cualquier funci√≥n p√∫blica, adem√°s de las otras puniciones judiciales que se aplicaran al caso. Por el art√≠culo 218 del Reglamento Interno de la C√°mara de los Diputados, cualquier ciudadano brasile√±o puede denunciar el Presidente de la Rep√∫blica, el Vicepresidente o Ministros por crimen de responsabilidad. Documentos deben acompa√±ar la denuncia o debe ser informado donde ellos pueden ser encontrados. Cuando el Presidente de la C√°mara recibe una denuncia, debe leerla en la sesi√≥n siguiente de la plenaria y despacharla a una comisi√≥n especial electa con representantes de todos los partidos, observada la respectiva proporci√≥n. La comisi√≥n especial tiene 48 horas para elegir su presidente y relator, y debe emitir su concepto en el plazo de cinco sesiones de la plenaria. Despu√©s de 48 horas de la publicaci√≥n del parecer de la comisi√≥n especial, debe ser sometido para votaci√≥n nominal en la pauta de votaciones de la plenaria, en la sesi√≥n siguiente. Si dos tercios de los congresistas aprueban la continuaci√≥n del proceso, la decisi√≥n es entonces avisada para el Presidente del Senado Federal, dentro del plazo de dos sesiones. Mientras que la atribuci√≥n no es legalmente obligatoria, ser√≠a agradecida.</td>\n",
       "      <td>¬øQui√©n resuelve acerca de las propuestas de moci√≥n de censura de acuerdo con las leyes de Brasil?</td>\n",
       "      <td>{'text': ['el Presidente de la C√°mara de Diputados'], 'answer_start': [432]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b6ec1785-3662-44c8-afa5-139f4b3f8bae</td>\n",
       "      <td>Esteban I de Hungr√≠a</td>\n",
       "      <td>Reinado \\nG√©za muri√≥ en 997 y Esteban convoc√≥ una asamblea en Esztergom donde sus participantes lo declararon gran pr√≠ncipe. Inicialmente, solo controlaba las regiones noroccidentales de la cuenca c√°rpata; el resto del territorio todav√≠a segu√≠a bajo el control de otros l√≠deres tribales. El ascenso de Esteban al trono se bas√≥ en el principio de primogenitura, que establece que un l√≠der tribal solo pod√≠a ser sucedido por su primer hijo var√≥n. Por otra parte, esto contradec√≠a la tradici√≥n de estos pueblos h√∫ngaros, seg√∫n la cual G√©za debi√≥ haber sido sucedido por el miembro de mayor edad de la dinast√≠a √Årpad, que en ese momento era Cupan (nombre latino de Kopp√°ny). Cupan, quien ostentaba el t√≠tulo de duque de Somogy, hab√≠a administrado durante muchos a√±os las regiones de Transdanubia al sur del lago Balat√≥n.</td>\n",
       "      <td>¬øCu√°ndo fue proclamado gran pr√≠ncipe Esteban I?</td>\n",
       "      <td>{'text': ['en 997'], 'answer_start': [20]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cd844a33-6ece-405f-a940-eba05342a108</td>\n",
       "      <td>Las Leonas le ganaron a Espa√±a en el Mundial 2010 de hockey</td>\n",
       "      <td>4 de septiembre de 2010 La selecci√≥n argentina de hockey, \"Las Leonas\", le ganaron a Espa√±a por 4 tantos contra 0 en una fr√≠a jornada en la ciudad de Rosario, Argentina. Jugaron vestidas con polleras y musculosas a 2 grados de temperatura ambiente. El primer tiempo el equipo argentino fue muy ofensivo llegando al arco en varias oportunidades. A los 12' Garc√≠a gan√≥ un corto que convirti√≥ Barrionuevo. Dos minutos despu√©s convirti√≥ la m√°xima goleadora del equipo una jugada empezada por Barrionuevo por el lateral izquierdo, continuada en el fondo de la cancha por Merino que tiro el centro. Espa√±a no pod√≠a ofrecer resistencia a este juego excepto algunas acciones de su capitana Camon que logr√≥ ganar metros en el campo y algunas subidas leves de Silvia Mu√±√≥z. La jugada m√°s peligrosa la tuvo la jugadora espa√±ola de nacionalidad argentina Montserrat Cruz que qued√≥ cara a cara con Succi y no pudo convertir. En la segunda parte la selecci√≥n argentina fue netamente superior, tuvo 7 de los 8 cortos que produjo en el partido, uno solo se convirti√≥ en tanto. Fue a los 12' tras un rebote en la defensora Ybarra despu√©s de un rechazo de la arquera. Ybarra genero una jugada de peligro para su equipo en un corner a los 8', ese corner fue el √∫nico que tuvo Espa√±a. Cuando la diferencia era de cuatro goles, el equipo argentino le dio descanso a sus titulares permitiendo jugar a las suplentes.</td>\n",
       "      <td>¬øPor cu√°nto han vencido las Leonas a Espa√±a en el partido de hockey?</td>\n",
       "      <td>{'text': ['por 4 tantos contra 0'], 'answer_start': [92]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd4718",
   "metadata": {},
   "source": [
    "## Preprocessing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "96049e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\sifigueroa/.cache\\huggingface\\hub\\models--BSC-TeMU--roberta-base-bne\\snapshots\\052845e3a3abcabb150e4724d2c85f0ab59dd67e\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"BSC-TeMU/roberta-base-bne\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50262\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\sifigueroa/.cache\\huggingface\\hub\\models--BSC-TeMU--roberta-base-bne\\snapshots\\052845e3a3abcabb150e4724d2c85f0ab59dd67e\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\sifigueroa/.cache\\huggingface\\hub\\models--BSC-TeMU--roberta-base-bne\\snapshots\\052845e3a3abcabb150e4724d2c85f0ab59dd67e\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\sifigueroa/.cache\\huggingface\\hub\\models--BSC-TeMU--roberta-base-bne\\snapshots\\052845e3a3abcabb150e4724d2c85f0ab59dd67e\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\sifigueroa/.cache\\huggingface\\hub\\models--BSC-TeMU--roberta-base-bne\\snapshots\\052845e3a3abcabb150e4724d2c85f0ab59dd67e\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\sifigueroa/.cache\\huggingface\\hub\\models--BSC-TeMU--roberta-base-bne\\snapshots\\052845e3a3abcabb150e4724d2c85f0ab59dd67e\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\sifigueroa/.cache\\huggingface\\hub\\models--BSC-TeMU--roberta-base-bne\\snapshots\\052845e3a3abcabb150e4724d2c85f0ab59dd67e\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"BSC-TeMU/roberta-base-bne\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50262\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5c3a51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2ce24ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 109, 8366, 4100, 49076, 356, 4680, 85, 2, 2, 23803, 356, 4680, 4100, 415, 12031, 1315, 340, 68, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01aa857",
   "metadata": {},
   "source": [
    "Dependiendo del modelo que seleccion√≥, ver√° diferentes claves en el diccionario devueltas por la celda de arriba. No importan mucho para lo que estamos haciendo aqu√≠ (solo sepa que son requeridos por el modelo que crearemos m√°s adelante), puede obtener m√°s informaci√≥n sobre ellos en este tutorial si est√° interesado.\n",
    "\n",
    "Ahora bien, una cosa espec√≠fica para el preprocesamiento en cuesti√≥n es c√≥mo tratar documentos muy largos. Por lo general, los truncamos en otras tareas, cuando son m√°s largos que la longitud m√°xima de oraci√≥n del modelo, pero aqu√≠, eliminar parte del contexto puede resultar en la p√©rdida de la respuesta que estamos buscando. Para lidiar con esto, permitiremos que un ejemplo (largo) en nuestro conjunto de datos proporcione varias caracter√≠sticas de entrada, cada una de una longitud m√°s corta que la longitud m√°xima del modelo (o la que establecemos como un hiperpar√°metro). Adem√°s, en caso de que la respuesta est√© en el punto en que dividimos un contexto largo, permitimos cierta superposici√≥n entre las caracter√≠sticas que generamos controladas por el hiperpar√°metro doc_stride:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c2415ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475dff6d",
   "metadata": {},
   "source": [
    "Encontremos el ejemplo m√°s largo en nuestro dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "36aded61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c6a72",
   "metadata": {},
   "source": [
    "Sin truncar obtenemos la siguiente longitud de los input IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a69091f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534d6c5",
   "metadata": {},
   "source": [
    "Ahora, truncamos (y perdemos informaci√≥n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2178e579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1ae48e",
   "metadata": {},
   "source": [
    "Tenga en cuenta que nunca queremos truncar la pregunta, solo el contexto, por eso usamos el truncamiento only_second. Ahora, nuestro tokenizador puede devolvernos autom√°ticamente una lista de caracter√≠sticas con un l√≠mite de cierta longitud m√°xima, con la superposici√≥n que hablamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "89e15ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e75c8",
   "metadata": {},
   "source": [
    "Ahora no tenemos una lista de input_ids, sino varias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f028953a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 240]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d1614",
   "metadata": {},
   "source": [
    "Y si los decodificamos, podemos ver la superposici√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ddc26fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2327dad",
   "metadata": {},
   "source": [
    "Ahora, esto nos dar√° algo de trabajo para tratar adecuadamente las respuestas: necesitamos encontrar en cu√°l de esas caracter√≠sticas se encuentra realmente la respuesta y d√≥nde exactamente en esa caracter√≠stica. Los modelos que usaremos requieren las posiciones inicial y final de estas respuestas en los tokens, por lo que tambi√©n necesitaremos mapear partes del contexto original a algunos tokens. Afortunadamente, el tokenizador que estamos usando puede ayudarnos con eso devolviendo un offset_mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1e11482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb0d00",
   "metadata": {},
   "source": [
    "Esto da, para cada √≠ndice de nuestro IDS de entrada, el car√°cter inicial y final correspondiente en el texto original que dio nuestro token. El primer token ([CLS]) tiene (0, 0) porque no corresponde a ninguna parte de la pregunta / respuesta, entonces el segundo token es el mismo que los caracteres 0 a 3 de la pregunta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "30d63714",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d04061",
   "metadata": {},
   "source": [
    "Entonces, podemos usar este mapeo para encontrar la posici√≥n de los tokens de inicio y finalizaci√≥n de nuestra respuesta en una caracter√≠stica determinada. Solo tenemos que distinguir qu√© partes de las compensaciones corresponden a la pregunta y qu√© parte corresponden al contexto, aqu√≠ es donde el m√©todo sequence_ids de nuestrotokenized_example puede ser √∫til:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f92b9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261002e",
   "metadata": {},
   "source": [
    "Devuelve None para los tokens especiales, luego 0 o 1 dependiendo de si el token correspondiente proviene de la primera oraci√≥n pasada (la pregunta) o de la segunda (el contexto). Ahora, con todo esto, podemos encontrar el primer y √∫ltimo token de la respuesta en una de nuestras funciones de entrada (o si la respuesta no est√° en esta funci√≥n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9a554609",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# Start token index of the current span in the text.\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# End token index of the current span in the text.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
    "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"The answer is not in this feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391485e1",
   "metadata": {},
   "source": [
    "Y podemos comprobar que es la respuesta correcta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c00632ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9c4b7",
   "metadata": {},
   "source": [
    "Para que este cuaderno funcione con cualquier tipo de modelo, debemos tener en cuenta el caso especial en el que el modelo espera relleno a la izquierda (en cuyo caso cambiamos el orden de la pregunta y el contexto):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0bc67752",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70043003",
   "metadata": {},
   "source": [
    "Ahora juntemos todo en una funci√≥n que aplicaremos a nuestro conjunto de entrenamiento. En el caso de respuestas imposibles (la respuesta est√° en otra caracter√≠stica dada por un ejemplo con un contexto largo), establecemos el √≠ndice cls tanto para la posici√≥n inicial como para la final. Tambi√©n podr√≠amos simplemente descartar esos ejemplos del conjunto de entrenamiento si la marca allow_impossible_answers esFalse. Dado que el preprocesamiento ya es lo suficientemente complejo como es, hemos mantenido que es simple para esta parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "92cd012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "       # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            \n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8392a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = prepare_train_features(datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "bf3ffa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\sifigueroa\\.cache\\huggingface\\datasets\\BSC-TeMU___sqac\\SQAC\\0.0.0\\039a4db4240c2eeaeb62497c826a49721ea57dffae7eeff93a8179fcdc5fd9fe\\cache-4f5c79361036e002.arrow\n",
      "Loading cached processed dataset at C:\\Users\\sifigueroa\\.cache\\huggingface\\datasets\\BSC-TeMU___sqac\\SQAC\\0.0.0\\039a4db4240c2eeaeb62497c826a49721ea57dffae7eeff93a8179fcdc5fd9fe\\cache-4f7c6f13b11f6db8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\sifigueroa\\.cache\\huggingface\\datasets\\BSC-TeMU___sqac\\SQAC\\0.0.0\\039a4db4240c2eeaeb62497c826a49721ea57dffae7eeff93a8179fcdc5fd9fe\\cache-d192a3c385fd19f2.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c436db76",
   "metadata": {},
   "source": [
    "## Fine-tuning del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0959a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\sifigueroa/.cache\\huggingface\\hub\\models--BSC-TeMU--roberta-base-bne\\snapshots\\052845e3a3abcabb150e4724d2c85f0ab59dd67e\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"BSC-TeMU/roberta-base-bne\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50262\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\sifigueroa/.cache\\huggingface\\hub\\models--BSC-TeMU--roberta-base-bne\\snapshots\\052845e3a3abcabb150e4724d2c85f0ab59dd67e\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at BSC-TeMU/roberta-base-bne were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at BSC-TeMU/roberta-base-bne and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "20c96bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-sqac\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4423482c",
   "metadata": {},
   "source": [
    "Luego, necesitaremos un data_collator que agrupe nuestros ejemplos procesados, aqu√≠ el predeterminado funcionar√°:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "25a9efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c444ab42",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Repository.__init__() got an unexpected keyword argument 'private'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[236], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:489\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# Create clone of distant repo and output directory if needed\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_git_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;66;03m# In case of pull, we need to make sure every process has the latest.\u001b[39;00m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:3284\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[1;34m(self, at_init)\u001b[0m\n\u001b[0;32m   3281\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m get_full_repo_name(repo_name, token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_token)\n\u001b[0;32m   3283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo \u001b[38;5;241m=\u001b[39m \u001b[43mRepository\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3287\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3289\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m   3291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moverwrite_output_dir \u001b[38;5;129;01mand\u001b[39;00m at_init:\n\u001b[0;32m   3292\u001b[0m         \u001b[38;5;66;03m# Try again after wiping output_dir\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: Repository.__init__() got an unexpected keyword argument 'private'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b05b779b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c2fed",
   "metadata": {},
   "source": [
    "Como el entrenamiento es largo, guardemos el modelo por si necesitamos reiniciar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a53978f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-squad-trained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"test-squad-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78767d0e",
   "metadata": {},
   "source": [
    "## Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00ca98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "    \n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf1e51",
   "metadata": {},
   "source": [
    "And like before, we can apply that function to our validation set easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e716abf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2310698800.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[50], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    remove_columns=datasets[\"test\"].column_names\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "validation_features = datasets[\"test\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"test\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5bede2",
   "metadata": {},
   "source": [
    "Now we can grab the predictions for all features by using the Trainer.predict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75085b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(validation_features)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "941e7c9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvalidation_features\u001b[49m\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mvalidation_features\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(validation_features\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validation_features' is not defined"
     ]
    }
   ],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "636efe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b2086aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m start_logits \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mstart_logits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      2\u001b[0m end_logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mend_logits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      3\u001b[0m offset_mapping \u001b[38;5;241m=\u001b[39m validation_features[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
    "# an example index\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "        # to part of the input_ids that are not in the context.\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928703b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"test\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7dc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "\n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795bc1e",
   "metadata": {},
   "source": [
    "And we can apply our post-processing function to our raw predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"test\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"test\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ahora puede cargar el resultado del entrenamiento en el Hub, simplemente ejecute esta instrucci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
